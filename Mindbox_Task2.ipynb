{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install pyspark==3.0.1 py4j==0.10.9"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1SV3NgZaLT0u",
    "outputId": "29dec3ae-e066-4568-d57f-741b13dc8141"
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.0.1 in c:\\users\\admin\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9 in c:\\users\\admin\\lib\\site-packages (0.10.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, asc, desc, regexp_replace\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkTest\").getOrCreate()\n",
    "\n",
    "# Метод с помощью, который возвращает все продукты с их категориями (датафрейм с набором всех пар «Имя продукта – Имя категории»).\n",
    "def result(df_p,df_c,df_pc):\n",
    "  df_rez = ((df_p.join(df_pc, df_p.id == df_pc.id_product, how='left_outer')).join(df_c, df_c.id == df_pc.id_category, how ='left_outer').orderBy('product_name','category_name')).select(col(\"product_name\").alias(\"Product\"), col(\"category_name\").alias(\"Category\"))\n",
    "  df_rez.show()\n",
    "  return df_rez\n",
    "\n",
    "# В датафреймах заданы продукты, категории и связь между ними.\n",
    "products_data = [(1, \"product1\"),\n",
    "                 (2, \"product2\"),\n",
    "                 (3, \"product3\"),\n",
    "                 (4, \"product4\")]\n",
    "categories_data = [(1, \"category1\"),\n",
    "                   (2, \"category2\"),\n",
    "                   (3, \"category3\"),\n",
    "                   (4, \"category4\")]\n",
    "products_categories_data = [(1, 1),\n",
    "                            (1, 2),\n",
    "                            (2, 1),\n",
    "                            (4, 3),\n",
    "                            (4, 2)]\n",
    "\n",
    "schema_p = StructType([StructField(\"id\", IntegerType(), False), StructField(\"product_name\", StringType(), False)])\n",
    "schema_c = StructType([StructField(\"id\", IntegerType(), False), StructField(\"category_name\", StringType(), False)])\n",
    "schema_pc = StructType([StructField(\"id_product\", IntegerType(), True), StructField(\"id_category\", IntegerType(), True)])\n",
    "\n",
    "df_p = spark.createDataFrame(data=products_data,schema=schema_p)\n",
    "df_c = spark.createDataFrame(data=categories_data,schema=schema_c)\n",
    "df_pc = spark.createDataFrame(data=products_categories_data,schema=schema_pc)\n",
    "\n",
    "result(df_p,df_c,df_pc)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VAQ_unSbT1yx",
    "outputId": "67c96af2-c53a-4195-cd9a-28a5ab9727d5"
   },
   "execution_count": 6,
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m col, asc, desc, regexp_replace\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession, DataFrame\n\u001B[1;32m----> 5\u001B[0m spark \u001B[38;5;241m=\u001B[39m \u001B[43mSparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappName\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mSparkTest\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Метод с помощью, который возвращает все продукты с их категориями (датафрейм с набором всех пар «Имя продукта – Имя категории»). \u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mresult\u001B[39m(df_p,df_c,df_pc):\n",
      "File \u001B[1;32m~\\lib\\site-packages\\pyspark\\sql\\session.py:186\u001B[0m, in \u001B[0;36mSparkSession.Builder.getOrCreate\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    184\u001B[0m         sparkConf\u001B[38;5;241m.\u001B[39mset(key, value)\n\u001B[0;32m    185\u001B[0m     \u001B[38;5;66;03m# This SparkContext may be an existing one.\u001B[39;00m\n\u001B[1;32m--> 186\u001B[0m     sc \u001B[38;5;241m=\u001B[39m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msparkConf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    187\u001B[0m \u001B[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[39;00m\n\u001B[0;32m    188\u001B[0m \u001B[38;5;66;03m# by all sessions.\u001B[39;00m\n\u001B[0;32m    189\u001B[0m session \u001B[38;5;241m=\u001B[39m SparkSession(sc)\n",
      "File \u001B[1;32m~\\lib\\site-packages\\pyspark\\context.py:376\u001B[0m, in \u001B[0;36mSparkContext.getOrCreate\u001B[1;34m(cls, conf)\u001B[0m\n\u001B[0;32m    374\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    375\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 376\u001B[0m         \u001B[43mSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mSparkConf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    377\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\n",
      "File \u001B[1;32m~\\lib\\site-packages\\pyspark\\context.py:133\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    129\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    130\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    131\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 133\u001B[0m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    134\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    135\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001B[0;32m    136\u001B[0m                   conf, jsc, profiler_cls)\n",
      "File \u001B[1;32m~\\lib\\site-packages\\pyspark\\context.py:325\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[1;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[0;32m    323\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    324\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_gateway:\n\u001B[1;32m--> 325\u001B[0m         SparkContext\u001B[38;5;241m.\u001B[39m_gateway \u001B[38;5;241m=\u001B[39m gateway \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mlaunch_gateway\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    326\u001B[0m         SparkContext\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_gateway\u001B[38;5;241m.\u001B[39mjvm\n\u001B[0;32m    328\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m instance:\n",
      "File \u001B[1;32m~\\lib\\site-packages\\pyspark\\java_gateway.py:105\u001B[0m, in \u001B[0;36mlaunch_gateway\u001B[1;34m(conf, popen_kwargs)\u001B[0m\n\u001B[0;32m    102\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.1\u001B[39m)\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(conn_info_file):\n\u001B[1;32m--> 105\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJava gateway process exited before sending its port number\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    107\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(conn_info_file, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m info:\n\u001B[0;32m    108\u001B[0m     gateway_port \u001B[38;5;241m=\u001B[39m read_int(info)\n",
      "\u001B[1;31mException\u001B[0m: Java gateway process exited before sending its port number"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
